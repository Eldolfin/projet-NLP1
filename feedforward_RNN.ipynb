{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f384ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import time\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "ds = load_dataset(\"AmazonScience/massive\", \"fr-FR\")\n",
    "class_name = \"scenario\"\n",
    "\n",
    "\n",
    "X_train = ds[\"train\"][\"utt\"]\n",
    "Y_train = ds[\"train\"][class_name]\n",
    "X_valid = ds[\"validation\"][\"utt\"]\n",
    "Y_valid = ds[\"validation\"][class_name]\n",
    "X_test = ds[\"test\"][\"utt\"]\n",
    "Y_test = ds[\"test\"][class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pretrained = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN ONLY IF YOU DON'T RUN THE PREVIOUS BLOCK, ELSE TAKES TOO MUCH MEMORY\n",
    "\n",
    "#fasttext.util.download_model('fr', if_exists='ignore')  \n",
    "#ft = fasttext.load_model('cc.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SCENARIOS = ['social', 'transport', 'calendar', 'play', 'news', 'datetime', 'recommendation', 'email',\n",
    "              'iot', 'general', 'audio', 'lists', 'qa', 'cooking', 'takeaway', 'music', 'alarm', 'weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(traindata, validdata):\n",
    "    X_train_tokenized = [nltk.word_tokenize(x) for x in traindata]\n",
    "    X_valid_tokenized = [nltk.word_tokenize(x) for x in validdata]\n",
    "\n",
    "    #Create the word corpus to create embeddings\n",
    "    corpus = Counter(list(itertools.chain(*X_train_tokenized)))\n",
    "    corpus = sorted(corpus,key=corpus.get,reverse=True)\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus)}\n",
    "\n",
    "    #Create the embeddings\n",
    "    X_train_embeddings = [[onehot_dict[word] for word in sentence] for sentence in X_train_tokenized]\n",
    "    #We use an abritrary value for unk words (last index) -> no info from these words\n",
    "    X_valid_embeddings = [[onehot_dict[word] if word in onehot_dict else len(onehot_dict) for word in sentence] for sentence in X_valid_tokenized]\n",
    "    return X_train_embeddings, X_valid_embeddings\n",
    "\n",
    "def get_w2v(traindata, validdata, vector_size=100, window=5, w2v_encoder=None, fasttext=False):\n",
    "    X_train_tokenized = [nltk.word_tokenize(x) for x in traindata]\n",
    "    X_valid_tokenized = [nltk.word_tokenize(x) for x in validdata]\n",
    "    if w2v_encoder == None:\n",
    "        w2v_encoder_model = Word2Vec(sentences=X_train_tokenized, vector_size=vector_size, window=window, min_count=1, workers=4)\n",
    "        w2v_encoder = w2v_encoder_model.wv\n",
    "    if fasttext:\n",
    "        vector_size = w2v_encoder.get_dimension()\n",
    "    else:\n",
    "        vector_size = w2v_encoder.vector_size\n",
    "    train_tensors = [torch.tensor([w2v_encoder[word] if word in w2v_encoder else [0.] * vector_size for word in x]) for x in X_train_tokenized]\n",
    "    valid_tensors = [torch.tensor([w2v_encoder[word] if word in w2v_encoder else [0.] * vector_size for word in x]) for x in X_valid_tokenized]\n",
    "    return train_tensors, valid_tensors\n",
    "\n",
    "\n",
    "def get_w2v_test(traindata, validdata, vector_size=100, window=5, w2v_encoder=None):\n",
    "    #X_train_tokenized = [[_SCENARIOS[y], \"<BOS>\"] + nltk.word_tokenize(x) for x,y in traindata]\n",
    "    #X_valid_tokenized = [[_SCENARIOS[y], \"<BOS>\"] + nltk.word_tokenize(x) for x,y in validdata]\n",
    "    X_train_tokenized = [nltk.tokenize.wordpunct_tokenize(x) for x in traindata]\n",
    "    X_valid_tokenized = [nltk.tokenize.wordpunct_tokenize(x) for x in validdata]\n",
    "    if w2v_encoder == None:\n",
    "        w2v_encoder_model = Word2Vec(sentences=X_train_tokenized, vector_size=vector_size, window=window, min_count=1, workers=4)\n",
    "        w2v_encoder = w2v_encoder_model.wv\n",
    "    train_tensors = [torch.tensor([w2v_encoder[word] if word in w2v_encoder else [0.] * w2v_encoder.get_dimension() for word in x[:-1]]) for x in X_train_tokenized if len(x) > 1]\n",
    "    valid_tensors = [torch.tensor([w2v_encoder[word] if word in w2v_encoder else [0.] * w2v_encoder.get_dimension() for word in x[:-1]]) for x in X_valid_tokenized if len(x) > 1]\n",
    "    train_targets_tensors = [torch.tensor([w2v_encoder[word] if word in w2v_encoder else [0.] * w2v_encoder.get_dimension() for word in x[1:]]) for x in X_train_tokenized if len(x) > 1]\n",
    "    valid_target_tensors = [torch.tensor([w2v_encoder[word] if word in w2v_encoder else [0.] * w2v_encoder.get_dimension() for word in x[1:]]) for x in X_valid_tokenized if len(x) > 1]\n",
    "    return train_tensors, train_targets_tensors, valid_tensors, valid_target_tensors\n",
    "\n",
    "def get_vocab_size():\n",
    "    X_train_tokenized = [nltk.word_tokenize(x) for x in X_train]\n",
    "    corpus = Counter(list(itertools.chain(*X_train_tokenized)))\n",
    "    return len(corpus)\n",
    "\n",
    "vocab_size = get_vocab_size()\n",
    "\n",
    "def collate_pack_onehot(batch):\n",
    "    data = [nn.functional.one_hot(item[0], num_classes=vocab_size + 1).float() for item in batch]\n",
    "    packed_data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    target = [item[1] for item in batch]\n",
    "    return packed_data, torch.tensor(target)\n",
    "\n",
    "def collate_pack(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    packed_data = nn.utils.rnn.pack_sequence(data, enforce_sorted=False)\n",
    "    target = [item[1] for item in batch]\n",
    "    return packed_data, torch.tensor(target)\n",
    "\n",
    "def createData(method, batch_size=25, w2v_vector_size=100, w2v_window=5):\n",
    "    if method == \"index\":\n",
    "        return get_index(X_train, X_valid)\n",
    "    \n",
    "    elif method == \"padded_index\":\n",
    "        embedded_train, embedded_valid = get_index(X_train, X_valid)\n",
    "        #padding embedding vectors to max sentence length\n",
    "        max_sentence_length = len(max(embedded_train, key=len))\n",
    "        X_train_padded = [np.pad(x, (0, max_sentence_length - len(x)), 'constant', constant_values=(0,0)) for x in embedded_train]\n",
    "\n",
    "        #truncate if needed before padding\n",
    "        embedded_valid = [x[:max_sentence_length] for x in embedded_valid]\n",
    "        X_valid_padded = [np.pad(x, (0, max_sentence_length - len(x)), 'constant', constant_values=(0,0)) for x in embedded_valid]\n",
    "        return X_train_padded, X_valid_padded\n",
    "    \n",
    "    elif method == \"onehot\":\n",
    "        embedded_train, embedded_valid = get_index(X_train, X_valid)\n",
    "        train_tensors = [torch.tensor(x) for x in embedded_train]\n",
    "        train_tensors = list(zip(train_tensors, torch.tensor(Y_train)))\n",
    "        valid_tensors = [torch.tensor(x) for x in embedded_valid]\n",
    "        valid_tensors = list(zip(valid_tensors, torch.tensor(Y_valid)))\n",
    "        trainloader = DataLoader(train_tensors, collate_fn=collate_pack_onehot, shuffle=True, batch_size=batch_size)\n",
    "        validloader = DataLoader(valid_tensors, collate_fn=collate_pack_onehot, batch_size=batch_size)\n",
    "        return trainloader, validloader\n",
    "\n",
    "    elif method == \"tf-idf\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train_tfidf = torch.Tensor(vectorizer.fit_transform(X_train).toarray())\n",
    "        X_valid_tfidf = torch.Tensor(vectorizer.transform(X_valid).toarray())\n",
    "        train_data_tfidf = TensorDataset(X_train_tfidf, torch.tensor(Y_train))\n",
    "        valid_data_tfidf = TensorDataset(X_valid_tfidf, torch.tensor(Y_valid))\n",
    "        trainloader = DataLoader(train_data_tfidf, shuffle=True, batch_size=batch_size)\n",
    "        validloader = DataLoader(valid_data_tfidf, batch_size=batch_size)\n",
    "        return trainloader, validloader\n",
    "    \n",
    "    elif method == \"word2vec\": #one input will be a packed list of tensors of size (len(sentence), w2v_model.vector_size)\n",
    "        train_tensors, valid_tensors = get_w2v(X_train, X_valid, w2v_vector_size, w2v_window)\n",
    "        train_tensors = list(zip(train_tensors, torch.tensor(Y_train)))\n",
    "        valid_tensors = list(zip(valid_tensors, torch.tensor(Y_valid)))\n",
    "        trainloader = DataLoader(train_tensors, shuffle=True, batch_size=batch_size, collate_fn=collate_pack)\n",
    "        validloader = DataLoader(valid_tensors, batch_size=batch_size, collate_fn=collate_pack)\n",
    "        return trainloader, validloader\n",
    "    \n",
    "    elif method == \"word2vec_mean\": #one input will be a tensor with size (w2v_model.vector_size)\n",
    "        train_tensors, valid_tensors = get_w2v(X_train, X_valid, w2v_vector_size, w2v_window)\n",
    "        train_tensors = [torch.mean(x, dim=0, dtype=float) if x.numel() != 0 else torch.zeros(w2v_vector_size) for x in train_tensors]\n",
    "        valid_tensors = [torch.mean(x, dim=0, dtype=float) if x.numel() != 0 else torch.zeros(w2v_vector_size) for x in valid_tensors]\n",
    "        train_data = TensorDataset(torch.stack(train_tensors), torch.tensor(Y_train))\n",
    "        valid_data = TensorDataset(torch.stack(valid_tensors), torch.tensor(Y_valid))\n",
    "        trainloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "        validloader = DataLoader(valid_data, batch_size=batch_size)\n",
    "        return trainloader, validloader\n",
    "    \n",
    "    elif method == \"word2vec_pretrained\":\n",
    "        train_tensors, valid_tensors = get_w2v(X_train, X_valid, w2v_encoder=w2v_pretrained)\n",
    "        train_tensors = list(zip(train_tensors, torch.tensor(Y_train)))\n",
    "        valid_tensors = list(zip(valid_tensors, torch.tensor(Y_valid)))\n",
    "        trainloader = DataLoader(train_tensors, shuffle=True, batch_size=batch_size, collate_fn=collate_pack)\n",
    "        validloader = DataLoader(valid_tensors, batch_size=batch_size, collate_fn=collate_pack)\n",
    "        return trainloader, validloader\n",
    "    \n",
    "    elif method == \"word2vec_pretrained_mean\":\n",
    "        train_tensors, valid_tensors = get_w2v(X_train, X_valid, w2v_encoder=w2v_pretrained)\n",
    "        train_tensors = [torch.mean(x, dim=0, dtype=float) if x.numel() != 0 else torch.zeros(w2v_pretrained.vector_size, dtype=float) for x in train_tensors]\n",
    "        valid_tensors = [torch.mean(x, dim=0, dtype=float) if x.numel() != 0 else torch.zeros(w2v_pretrained.vector_size, dtype=float) for x in valid_tensors]\n",
    "        train_data = TensorDataset(torch.stack(train_tensors), torch.tensor(Y_train))\n",
    "        valid_data = TensorDataset(torch.stack(valid_tensors), torch.tensor(Y_valid))\n",
    "        trainloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "        validloader = DataLoader(valid_data, batch_size=batch_size)\n",
    "        return trainloader, validloader\n",
    "    \n",
    "    elif method == \"word2vec_pretrained_ft\":\n",
    "        train_tensors, valid_tensors = get_w2v(X_train, X_valid, w2v_encoder=ft, fasttext=True)\n",
    "        train_tensors = list(zip(train_tensors, torch.tensor(Y_train)))\n",
    "        valid_tensors = list(zip(valid_tensors, torch.tensor(Y_valid)))\n",
    "        trainloader = DataLoader(train_tensors, shuffle=True, batch_size=batch_size, collate_fn=collate_pack)\n",
    "        validloader = DataLoader(valid_tensors, batch_size=batch_size, collate_fn=collate_pack)\n",
    "        return trainloader, validloader\n",
    "    \n",
    "    elif method == \"generation\":\n",
    "        train_tensors, train_targets, valid_tensors, valid_targets = get_w2v_test(X_train, X_valid, w2v_encoder=ft)\n",
    "        train_data = list(zip(train_tensors, train_targets))\n",
    "        valid_data = list(zip(valid_tensors, valid_targets))\n",
    "        return train_data, valid_data\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"createData: method not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definitions\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation_function):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation_function = activation_function\n",
    "    \n",
    "    def forward(self, text):\n",
    "        hidden = self.fc1(text.float())\n",
    "        hidden = self.activation_function(hidden)\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.fc2(hidden)\n",
    "        return out\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, bidirectional=bidirectional, batch_first=True, num_layers=n_layers, dropout=0.5)\n",
    "        self.h2o = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, text):\n",
    "        rnn_output, hidden = self.rnn(text)\n",
    "        out = self.h2o(hidden[0])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "class GenRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.i2h = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.i2o = nn.Linear(input_dim + hidden_dim, output_dim)\n",
    "        self.o2o = nn.Linear(hidden_dim + output_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, text, hidden):\n",
    "        input_combined = torch.cat((text, hidden))\n",
    "        hidden = torch.tanh(self.i2h(input_combined))\n",
    "        output = torch.tanh(self.i2o(input_combined))\n",
    "        output_combined = torch.cat((hidden, output))\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_dim)\n",
    "    \n",
    "#essayer apres de batch et utiliser rnn en ajoutant la loss de chaque etape retournee par self.rnn\n",
    "class GenRNN_test(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, bidirectional=bidirectional, batch_first=True, num_layers=n_layers, dropout=0.5)\n",
    "        self.h2o = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, text):\n",
    "        rnn_output, hidden = self.rnn(text)\n",
    "        out = self.h2(rnn_output)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate a model's predictions. Returns (loss, accuracy)\n",
    "def evaluate(model, dataloader, criterion=nn.CrossEntropyLoss()):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sentences, labels in dataloader:\n",
    "        outputs = model.forward(sentences.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #Calculate correct predictions for this batch\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += len(predictions)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a74247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, n_epoch, learning_rate, report_every = 1, criterion = nn.CrossEntropyLoss(), optimizer = torch.optim.SGD, \n",
    "          needsOnehot = False, eval=False, validloader=None):\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "    vocab_size = -1\n",
    "    if needsOnehot:\n",
    "        vocab_size = get_vocab_size()\n",
    "\n",
    "    #print(f\"training on data set with n_batches = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        model.zero_grad() # clear the gradients\n",
    "\n",
    "        for sentences, labels in training_data:\n",
    "            input_tensor = sentences\n",
    "            if needsOnehot:\n",
    "                input_tensor = nn.functional.one_hot(sentences, num_classes=vocab_size + 1)\n",
    "            output = model.forward(input_tensor.float())\n",
    "            loss = criterion(output, labels)\n",
    "            # optimize parameters\n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        scheduler.step(current_loss)\n",
    "        if eval:\n",
    "            valid_loss, valid_acc = evaluate(model, validloader)\n",
    "            model.train()\n",
    "            all_losses.append((current_loss / len(training_data), valid_loss, valid_acc))\n",
    "        else:\n",
    "            all_losses.append(current_loss / len(training_data))\n",
    "        if report_every != 0 and iter % report_every == 0:\n",
    "            if eval:\n",
    "                lr = optimizer.param_groups[0][\"lr\"]\n",
    "                print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1][0]}  ||  validation: loss = {all_losses[-1][1]}, accuracy = {all_losses[-1][2]}, lr = {lr}\")\n",
    "            else:\n",
    "                print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34820c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = createData(\"tf-idf\", 25)\n",
    "model = Feedforward(len(trainloader.dataset[0][0]), 100, 18, nn.functional.relu)\n",
    "\n",
    "ff_tfidf_results = train(model, trainloader, 100, 1e-4, optimizer=torch.optim.Adam, eval=True, validloader=validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ce34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = createData(\"onehot\")\n",
    "\n",
    "rnn = RNN(vocab_size + 1, 500, 18, 3)\n",
    "rnn_onehot_results = train(rnn, trainloader, 100, 1e-4, eval=True, validloader=validloader, optimizer=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ab26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = createData(\"word2vec\", batch_size=25, w2v_vector_size=100, w2v_window=5)\n",
    "rnn_w2v = RNN(trainloader.dataset[0][0].size()[1], 100, 18, 1, bidirectional=True)\n",
    "rnn_w2v_results = train(rnn_w2v, trainloader, 100, 0.1, eval=True, validloader=validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400542bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = createData(\"word2vec_mean\", batch_size=25, w2v_vector_size=100, w2v_window=5)\n",
    "model = Feedforward(len(trainloader.dataset[0][0]), 1000, 18, nn.functional.relu)\n",
    "\n",
    "FF_w2v_mean_results = train(model, trainloader, 100, 0.001, optimizer=torch.optim.Adam, eval=True, validloader=validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = createData(\"word2vec_pretrained_mean\", batch_size=25)\n",
    "model = Feedforward(len(trainloader.dataset[0][0]), 1000, 18, nn.functional.relu)\n",
    "\n",
    "FF_w2v_pretrained_mean_results = train(model, trainloader, 100, 0.001, optimizer=torch.optim.Adam, eval=True, validloader=validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = createData(\"word2vec_pretrained\", batch_size=25)\n",
    "rnn_w2v = RNN(trainloader.dataset[0][0].size()[1], 100, 18, 1, bidirectional=False)\n",
    "rnn_w2v_pretrained_results = train(rnn_w2v, trainloader, 20, 0.05, eval=True, validloader=validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ec7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = createData(\"word2vec_pretrained_ft\", batch_size=25)\n",
    "rnn_w2v_ft = RNN(trainloader.dataset[0][0].size()[1], 100, 18, 4, bidirectional=False)\n",
    "rnn_w2v_ft_pretrained_results = train(rnn_w2v_ft, trainloader, 100, 0.01, eval=True, validloader=validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bf1b1",
   "metadata": {},
   "source": [
    "# Models evaluations:\n",
    "\n",
    "### RNN:\n",
    "| Model     | hidden_size | learning_rate | layers | epochs | train_loss | valid_loss | valid_accuracy | test_loss | test_accuracy | optimizer\n",
    "| --------- | ----------- | ------------- | ------ | ------ | ---------- | ---------- | -------------- | --------- | ------------- | ---------\n",
    "| onehot    | 100         |  0.01         |   4    |  30    | 0.0094407  | 0.80427022 | 0.8440727988   |           |               | Adam\n",
    "| w2v       | 100         |  0.1          |   4    |  30    | 0.6402964  | 1.66495737 | 0.5533694048   |           |               | SGD\n",
    "| w2v_google| 100         |  0.01         |   4    |  100   | 0.4981368  | 0.84794813 | 0.7668470241   |           |               | SGD\n",
    "| w2v_fasttext| 100       |  0.01         |   1    |  200   | 0.1366848  | 0.89333971 | 0.7899655681   |           |               | SGD\n",
    "\n",
    "##### Notes:\n",
    "- for onehot, overfitting is a problem, especially after 30 epochs where the validation loss starts to increase\n",
    "\n",
    "### FeedForward:\n",
    "| Model     | hidden_size | learning_rate | epochs | train_loss | valid_loss | valid_accuracy | test_loss | test_accuracy | optimizer\n",
    "| --------- | ----------- | ------------- | ------ | ---------- | ---------- | -------------- | --------- | ------------- | ---------\n",
    "| tf-idf    | 100         |  1e-4         |  100   | 0.0910010  | 0.40158849 | 0.8878504672   |           |               | Adam\n",
    "| w2v_mean  | 1000        |  0.001        |  100   | 1.6188243  | 1.52717660 | 0.5371372356   |           |               | Adam\n",
    "| w2v_mean  | 1000        |  0.001        |  100   | 0.3744488  | 1.26549924 | 0.7412690605   |           |               | Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39296b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(model, training_data, n_epoch, learning_rate, report_every = 1, criterion = nn.CrossEntropyLoss(), optimizer = torch.optim.SGD, \n",
    "          needsOnehot = False, eval=False, validloader=None):\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "    vocab_size = -1\n",
    "    if needsOnehot:\n",
    "        vocab_size = get_vocab_size()\n",
    "\n",
    "    #print(f\"training on data set with n_batches = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        model.zero_grad() # clear the gradients\n",
    "\n",
    "        for inputs, targets in training_data:\n",
    "            hidden = model.initHidden()\n",
    "            loss = 0\n",
    "            for i in range(len(inputs)):\n",
    "                output, hidden = model.forward(inputs[i].float(), hidden)\n",
    "                loss_temp = criterion(output, targets[i])\n",
    "                loss += loss_temp\n",
    "                \n",
    "            # optimize parameters\n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        scheduler.step(current_loss)\n",
    "        if eval:\n",
    "            valid_loss, valid_acc = evaluate(model, validloader)\n",
    "            model.train()\n",
    "            all_losses.append((current_loss / len(training_data), valid_loss, valid_acc))\n",
    "        else:\n",
    "            all_losses.append(current_loss / len(training_data))\n",
    "        if report_every != 0 and iter % report_every == 0:\n",
    "            if eval:\n",
    "                lr = optimizer.param_groups[0][\"lr\"]\n",
    "                print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1][0]}  ||  validation: loss = {all_losses[-1][1]}, accuracy = {all_losses[-1][2]}, lr = {lr}\")\n",
    "            else:\n",
    "                print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata, validdata = createData(\"generation\", batch_size=25)\n",
    "rnn_gen = GenRNN(len(traindata[0][0][0]), 100, len(traindata[0][0][0]), 1, bidirectional=False)\n",
    "\n",
    "train_gen(rnn_gen, traindata, 20, 0.05, eval=False, validloader=validdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft[\"\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
